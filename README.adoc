= OpenShift User Workload Monitoring with Grafana

== Overview

OpenShift comes with a https://docs.openshift.com/container-platform/4.11/monitoring/monitoring-overview.html[User Workload Monitoring] feature that can be used to collect _Prometheus_ metrics directly from deployed applications, similarly how metrics are collected from the platform itself. Platform and user metrics can both be queried through a joint Thanos Querier endpoint that's also exposed by a Route.

The default _Observe_ section of the OpenShift UI provides dashboards for the platform metrics, but user metrics can only be queried by _PromQL_ queries. This example shows how to use _Grafana_ to access these metrics and use them on dashbards.

There are two different ways to access these metrics:

- Deploy a cluster-scoped Grafana that can access metrics from the whole cluster - cluster-admin pemission is required
- Deploy namespace-scoped Grafana instances that can access metrics for one namespace only - permission to the namespace is enough

[NOTE]
Minimum OpenShift v4.10 is required for the Grafana dashboards with variables based on `label_values()`, the related api endpoints are not enabled in earlier versions.

== Installation

=== Enable User Workload Monitoring

First we need to https://docs.openshift.com/container-platform/4.11/monitoring/enabling-monitoring-for-user-defined-projects.html[enable] User Workload Monitoring by creating the related link:kustomize/env/openshift-monitoring[ConfigMaps].

```
oc apply -k kustomize/env/openshift-monitoring
```

To test that user workload monitoring is up and running try to hit the internal or external endpoint:

```
# From outside the cluster - HTTP 401 Unauthorized is expected
curl -vk 'https://federate-openshift-user-workload-monitoring.apps.[cluster domain]/federate'

# From a Pod inside the cluster - HTTP 401 Unauthorized is expected
curl -vk 'https://prometheus-user-workload.openshift-user-workload-monitoring.svc:9092/federate'
```

=== Deploy a cluster-scoped Grafana

Deploy a Grafana in `grafana-monitoring` namespace. Create a _ServiceAccount_ (`grafana-thanos`) that has cluster-scoped `cluster-monitoring-view` permission (_ClusterRoleBinding_) and add its token as `Bearer` header in the _GrafanaDatasource_.

```
oc apply -k kustomize/env/grafana-monitoring-operator

SATOKEN=`oc sa get-token grafana-thanos -n grafana-monitoring`
sed -i '' "s/Bearer .*/Bearer $SATOKEN/" kustomize/env/grafana-monitoring/kustomization.yaml
oc apply -k kustomize/env/grafana-monitoring
```

[NOTE]
In OpenShift v4.11 `oc get-token` is deprected. Use `oc create token grafana-thanos --duration=9000h -n grafana-monitoring`  or `oc extract -n grafana-monitoring $(oc get secret -oname -n grafana-monitoring | grep grafana-thanos-token) --keys=token --to=-`

Note:

- This _GrafanaDatasource_ connects to endpoint `thanos-querier.openshift-monitoring.svc.cluster.local` on port *9091*, that requires cluster-scoped `cluster-monitoring-view` permission (for the _ServiceAccount_)
- The _ServiceAccount_ token is added in _GrafanaDatasource_ as:
  
  ```
    secureJsonData:
      httpHeaderValue1: >-
        Bearer [use grafana-thanos token]
  ```

- Access to Grafana UI requires cluster scoped permission by setting `-openshift-sar={"resource": "namespaces", "verb": "get"}`. This could be less restrictive and changed to let login anyone with access to the `grafana-monitoring` namespace by setting something like `-openshift-sar={"resource": "services", "verb": "get", "namespace":"grafana-monitoring"}`

=== Deploy team specific namespaces

Create namespaces `team-a` and `team-b` with an _OperatorGroup_ to enable operator installation and give `admin` and `monitoring-edit` permissions to "user1" and "user2":

```
oc apply -k kustomize/env/team-a-namespace
oc apply -k kustomize/env/team-b-namespace
```

[NOTE]
At this point we can switch to "user1" to manage "team-a" and "user2" for "team-b" as cluster-scoped permissions are not needed to install operators and deploy Grafana with namespace-scoped access to metrics. This means that application teams in an enterprise environment can deploy this solution in their own namespaces without assistance required from cluster admins.

Deploy Grafana and our applications to monitor (using Red Hat SSO and AMQ Broker in this example for demonstration purposes):

```
oc apply -k kustomize/env/team-a-operators

SATOKEN=`oc sa get-token grafana-thanos -n team-a`
sed -i '' "s/Bearer .*/Bearer $SATOKEN/" kustomize/env/team-a-workload/kustomization.yaml
oc apply -k kustomize/env/team-a-workload
```

And similarly for `team-b`:

```
oc apply -k kustomize/env/team-b-operators

SATOKEN=`oc sa get-token grafana-thanos -n team-b`
sed -i '' "s/Bearer .*/Bearer $SATOKEN/" kustomize/env/team-b-workload/kustomization.yaml
oc apply -k kustomize/env/team-b-workload
```

Note:

- These _GrafanaDatasources_ connect to endpoint `thanos-querier.openshift-monitoring.svc.cluster.local` on port *9092*, that requires only namespace scoped permission (for the _ServiceAccount_)
- But this endpoint also requires a matching `namespace=team-a` criteria in every Promtheus query and lookup, so we need to add `customQueryParameters: namespace=team-a`, so it's automatically added to the sent queries. (Though we actually have "namespace" filter in our queries used on the dashboards in this example)
- The `Bearer` token used belongs to the _ServiceAccount_ having only namespace scoped `view` permission (_RoleBinding_)
- Grafana UI requires only namespace scoped access to login: `-openshift-sar={"resource": "services", "verb": "get", "namespace":"team-a"}`

[NOTE]
If you have `NetworkPolicy` in your application namespaces, make sure incoming traffic from namespace `openshift-user-workload-monitoring` is enabled.

== Grafana 

Open the URL of created Grafana Routes (run `oc get route -ojsonpath='{$.spec.host}' grafana-route`) in all three namespaces and access one of the three deployed dashboard (AMQ, JMX, SSO):

- grafana-monitoring: Both namespaces shows up in the _Namespace_ selector, but needs cluster level permission to login
- team-a: Only metrics from namespace "team-a" shows up, but _user1_ can login
- team-b: Only metrics from namespace "team-b" shows up, but _user2_ can login



== Additional info

Related blog: https://cloud.redhat.com/blog/thanos-querier-versus-thanos-querier

To see the difference between the thanos-querier endpoints on port 9091 and 9092 we can run some curl commands. Port 9091 is exposed by a Route, for 9092 we can do port-forward:

```
BEARER_CLUSTER="$(oc sa get-token -n grafana-monitoring grafana-thanos)"
BEARER_TEAMA="$(oc sa get-token -n team-a grafana-thanos)"
BEARER_TEAMB="$(oc sa get-token -n team-b grafana-thanos)"

# Cluster scoped endpoint
curl -vk -H "Authorization: Bearer $BEARER_CLUSTER" 'https://thanos-querier-openshift-monitoring.apps.[cluster domain]/api/v1/query?query=up'

# Namespace scoped endpoint - the "namespace" filter is required
oc port-forward -n openshift-monitoring service/thanos-querier 9092 9092
curl -vk -H "Authorization: Bearer $BEARER_TEAMA" 'https://localhost:9092/api/v1/query?query=up&namespace=team-a'
curl -vk -H "Authorization: Bearer $BEARER_TEAMB" 'https://localhost:9092/api/v1/query?query=up&namespace=team-b'
```

Instead of a _ServiceAccount_ token we can also use our own user token (`oc whoami -t`) as Bearer header.

== Alerting

[NOTE]
The alerting related configuration was actually deployed by the scripts we run above, but this section describes the details around our custom alerts.

=== Alerting Rules

OpenShift comes with default platform alerts defined in the `openshift-*` namespaces. See _Alerting rules_ on the UI or run `oc get PrometheusRules -A -oyaml | grep 'alert:'`. These alerts are evaluated by Prometheus running in the `openshift-monitoring` namespace and should not be modified.

// OpenShift v4.11 has AlertingRule for custom rules: https://github.com/openshift/api/blob/master/monitoring/v1alpha1/0000_50_monitoring_01_alertingrules.crd.yaml

To configure https://docs.openshift.com/container-platform/4.11/monitoring/managing-alerts.html#managing-alerting-rules-for-user-defined-projects_managing-alerts[alerts for User Workload Monitoring metrics] we create `PrometheusRule` resources in our namespaces (`team-a`,`team-b`). The Keycloak operator deploys these by default, for AMQ see link:kustomize/base/amq/instance/alerts.yaml[].

These rules are evaluated by _Thanos Ruler_ in `openshift-user-workload-monitoring` namespace, so we can use _User_ and _Platform_ metrics too in the expressions. The actual rule config snippets generated by the operator from PrometheusRules are stored in a ConfigMap, try `oc describe ConfigMap -n openshift-user-workload-monitoring -l thanos-ruler-name=user-workload`. These custom _User_ alerting rules are visible on the OpenShift UI in _Developer_ view on the _Observe / Alerts_ page.

For details about rules see https://docs.openshift.com/container-platform/4.11/rest_api/monitoring_apis/prometheusrule-monitoring-coreos-com-v1.html[PrometheusRule spec] and https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[Prometheus doc].


[NOTE]
The `namespace` filter is automatically added to all metrics used in the expressions, which makes the _PrometheusRule_ resources usable in any namespaces, alerts will be based only on metrics coming from that namespace. The `namespace` (and `alertname`) label is also automatically added to the alerts when they fire, so alerts can be grouped by namespace for receivers.

ClusterRole `monitoring-rules-edit` is required in the namespace to create _PrometheusRules_. 

=== Notifications

Receivers for the standard platform alerts can be configured using the https://docs.openshift.com/container-platform/4.11/monitoring/managing-alerts.html#configuring-alert-receivers_managing-alerts[OpenShift UI] or editing the https://docs.openshift.com/container-platform/4.11/monitoring/managing-alerts.html#applying-custom-alertmanager-configuration_managing-alerts[alertmanager-main] Secret directly. The easiest is to configure a `Default` receiver to get all alerts. Alerts are grouped by namespace, with a default `group_interval: 30s` (initial wait) `group_interval: 5m` (wait time before notifications about changes) and `repeat_interval: 12h` (time before repeating an unchanged notification), see https://prometheus.io/docs/alerting/latest/configuration/#route[details].

While the _User_ alerts are also included in the _Default_ alert notifications, it's not practical to send _User_ alerts (relevant to application teams) to the same channels as _Platform_ alerts (relevant for cluster admins). Fortunately we can define custom, namespace scoped alert notification routes by creating `AlertmanagerConfigs`.

We need to enable this custom alerting feature by adding `enableUserAlertmanagerConfig: true` to `cluster-monitoring-config` ConfigMap:

```
  config.yaml: |
    ...
    alertmanagerMain:
      enableUserAlertmanagerConfig: true
```

Then we can create _AlertmanagerConfigs_ in our namespaces (`team-a`,`team-b`), for example:

```
apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
metadata:
  name: alert-notifications
spec:
  route:
    receiver: default
    groupWait: 30s
    groupInterval: 5m
    repeatInterval: 12h
  receivers:
  - name: default
    webhookConfigs:
    - url: https://webhook.example.com/
```

The alert notifications are sent by _Alertmanager_ in the `openshift-monitoring` namespace (unless separate _Alertmanager_ instance is enabled for user-defined alerts).
The actual alerting route config snippets generated by the operator from _AlertmanagerConfigs_ (merged with the cluster level `alertmanager-main` config) are stored in a Secret, try `oc extract  -n openshift-monitoring --to=- secret/alertmanager-main-generated`.

For details about alerting routes see https://docs.openshift.com/container-platform/4.11/rest_api/monitoring_apis/alertmanagerconfig-monitoring-coreos-com-v1beta1.html[AlertmanagerConfig spec] and https://prometheus.io/docs/alerting/latest/configuration/[Prometheus doc].

Note:

* If an _AlertmanagerConfig_ is created in the namespace, the _Default_ notification channel is not used anymore for user alerts in that namespace
* Group by `namespace` is automatically added, but we can add additional labels (e.g. `pod`) to get separate notifications accordingly
* Matchers for `namespace` is automatically added, so custom _AlertmanagerConfigs_ route only alerts from the namespace they were created in
* We can have multiple _AlertmanagerConfigs_ in a namespace, they are merged together
* For a more complex _AlertmanagerConfig_ with sub-routes see link:kustomize/env/team-b-workload/alertmanagerconfig.yaml[].

ClusterRole `alert-routing-edit` is required in the namespace to create _AlertmanagerConfigs_. 

=== Separate Alertmanager for user-defined alerts

For additional separation of alert notifications coming from _User_ and _Platform_ alerts, a https://docs.openshift.com/container-platform/4.11/monitoring/enabling-alert-routing-for-user-defined-projects.html#enabling-a-separate-alertmanager-instance-for-user-defined-alert-routing_enabling-alert-routing-for-user-defined-projects[separate Alertmanager instance] can be deployed in the `openshift-user-workload-monitoring` namespace. This is a good practice to handle configuration problems, performance issues or to separate access control.
